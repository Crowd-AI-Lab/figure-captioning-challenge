{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "KpTSn3ufJGQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4c9546-ecdd-4b77-c064-5cefc95000ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.65.0)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=cf6fe0dde49f6c6f9d1169872ef3c872e41a55d452ea138fb5b5d5e75e7acd9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vpi21-3GjKf",
        "outputId": "c37455d8-b4ba-4837-b648-0c84e7b86f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import sys\n",
        "import os\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import json\n",
        "import codecs\n",
        "json_load = lambda x: json.load(codecs.open(x, 'r', encoding='utf-8'))\n",
        "json_dump = lambda d, p: json.dump(d, codecs.open(p, 'w', 'utf-8'), indent=2, ensure_ascii=False)\n",
        "import numpy as np\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from rouge_score import rouge_scorer\n",
        "from scipy import interpolate\n",
        "from pathlib import Path\n",
        "from typing import List, Union, Iterable\n",
        "from itertools import zip_longest\n",
        "from collections import defaultdict\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_map = {\n",
        "    \"rouge1\": [{\"length\":3.9826927517,\"score\":0.0682902912},\n",
        "{\"length\":5.9549255862,\"score\":0.0978827539},\n",
        "{\"length\":7.9076051135,\"score\":0.1224228823},\n",
        "{\"length\":9.81988077,\"score\":0.1420500099},\n",
        "{\"length\":11.6670102227,\"score\":0.15667295},\n",
        "{\"length\":13.4282499633,\"score\":0.1688965384},\n",
        "{\"length\":15.0784735196,\"score\":0.1782539613},\n",
        "{\"length\":16.6026847751,\"score\":0.1855030362},\n",
        "{\"length\":18.0075526354,\"score\":0.1913154379},\n",
        "{\"length\":19.2746489221,\"score\":0.1960990131},\n",
        "{\"length\":20.4229391885,\"score\":0.1993243852},\n",
        "{\"length\":21.4450618191,\"score\":0.2024686698},\n",
        "{\"length\":22.3082453452,\"score\":0.204501986},\n",
        "{\"length\":23.1111106446,\"score\":0.2062906734},\n",
        "{\"length\":28.2587921661,\"score\":0.2123504881},\n",
        "{\"length\":55.718749344,\"score\":0.2515012516},\n",
        "{\"length\":81.5821532778,\"score\":0.2560368626},\n",
        "{\"length\":105.2805852348,\"score\":0.2501756483},\n",
        "{\"length\":126.637654023,\"score\":0.2424150409},\n",
        "{\"length\":145.7091626608,\"score\":0.2348272003},\n",
        "{\"length\":162.4257730011,\"score\":0.2280278165},\n",
        "{\"length\":177.1731711413,\"score\":0.2224555269},\n",
        "{\"length\":190.1122798547,\"score\":0.2179352276},\n",
        "{\"length\":201.4505573165,\"score\":0.2142724833}],\n",
        "    \"rouge2\":[{\"length\":3.9826927517,\"score\":0.0144407758},\n",
        "{\"length\":5.9549255862,\"score\":0.0249529835},\n",
        "{\"length\":7.9076051135,\"score\":0.0347267516},\n",
        "{\"length\":9.81988077,\"score\":0.0427076786},\n",
        "{\"length\":11.6670102227,\"score\":0.0485375859},\n",
        "{\"length\":13.4282499633,\"score\":0.0535693814},\n",
        "{\"length\":15.0784735196,\"score\":0.0572324993},\n",
        "{\"length\":16.6026847751,\"score\":0.0602716788},\n",
        "{\"length\":18.0075526354,\"score\":0.0628290212},\n",
        "{\"length\":19.2746489221,\"score\":0.0649507089},\n",
        "{\"length\":20.4229391885,\"score\":0.0661721408},\n",
        "{\"length\":21.4450618191,\"score\":0.0678942278},\n",
        "{\"length\":22.3082453452,\"score\":0.0688855871},\n",
        "{\"length\":23.1111106446,\"score\":0.0697352193},\n",
        "{\"length\":28.2587921661,\"score\":0.0736741358},\n",
        "{\"length\":55.718749344,\"score\":0.0946583707},\n",
        "{\"length\":81.5821532778,\"score\":0.1034647258},\n",
        "{\"length\":105.2805852348,\"score\":0.1066520132},\n",
        "{\"length\":126.637654023,\"score\":0.1083127966},\n",
        "{\"length\":145.7091626608,\"score\":0.1088493736},\n",
        "{\"length\":162.4257730011,\"score\":0.1086376524},\n",
        "{\"length\":177.1731711413,\"score\":0.1083631105},\n",
        "{\"length\":190.1122798547,\"score\":0.1080597696},\n",
        "{\"length\":201.4505573165,\"score\":0.1077033711}],\n",
        "\"rougeL\":[{\"length\":3.9826927517,\"score\":0.0638302511},\n",
        "{\"length\":5.9549255862,\"score\":0.0885365378},\n",
        "{\"length\":7.9076051135,\"score\":0.1080729605},\n",
        "{\"length\":9.81988077,\"score\":0.1226784665},\n",
        "{\"length\":11.6670102227,\"score\":0.13304084},\n",
        "{\"length\":13.4282499633,\"score\":0.1412546317},\n",
        "{\"length\":15.0784735196,\"score\":0.1473504772},\n",
        "{\"length\":16.6026847751,\"score\":0.1517939352},\n",
        "{\"length\":18.0075526354,\"score\":0.1552496947},\n",
        "{\"length\":19.2746489221,\"score\":0.1581296506},\n",
        "{\"length\":20.4229391885,\"score\":0.1597121746},\n",
        "{\"length\":21.4450618191,\"score\":0.1616429679},\n",
        "{\"length\":22.3082453452,\"score\":0.162688815},\n",
        "{\"length\":23.1111106446,\"score\":0.163630746},\n",
        "{\"length\":28.2587921661,\"score\":0.1666532163},\n",
        "{\"length\":55.718749344,\"score\":0.1846472648},\n",
        "{\"length\":81.5821532778,\"score\":0.1836643033},\n",
        "{\"length\":105.2805852348,\"score\":0.1785305626},\n",
        "{\"length\":126.637654023,\"score\":0.1737320905},\n",
        "{\"length\":145.7091626608,\"score\":0.1693876728},\n",
        "{\"length\":162.4257730011,\"score\":0.1657265038},\n",
        "{\"length\":177.1731711413,\"score\":0.162906589},\n",
        "{\"length\":190.1122798547,\"score\":0.1606680241},\n",
        "{\"length\":201.4505573165,\"score\":0.1588698335}]\n",
        "}"
      ],
      "metadata": {
        "id": "ySkkZZ4cJAHL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScicapSummarizationEval():\n",
        "    def __init__(self):\n",
        "        self.metric_list = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
        "        self.rouge = rouge_scorer.RougeScorer(\n",
        "            self.metric_list,\n",
        "            use_stemmer=True,\n",
        "        )\n",
        "\n",
        "        # build score normalization\n",
        "        \n",
        "        self.normalization_funcs = {}\n",
        "        for metric in self.metric_list:\n",
        "            random_data = rouge_map[metric]\n",
        "            length_list = np.array([d[\"length\"] for d in random_data])\n",
        "            score_list = np.array([d[\"score\"] for d in random_data])\n",
        "            normalization_func = interpolate.interp1d(\n",
        "                length_list,\n",
        "                score_list,\n",
        "                bounds_error=False,\n",
        "                fill_value=\"extrapolate\",\n",
        "            )\n",
        "            self.normalization_funcs[metric] = normalization_func\n",
        "        \n",
        "    def clean_text(self, text):\n",
        "        return text.lower()\n",
        "\n",
        "    def evaluate(\n",
        "        self, \n",
        "        hypothesis: List[str],\n",
        "        reference: List[str],\n",
        "    ):\n",
        "        assert len(hypothesis) == len(reference), \"The number of hypothesis and reference must be the same.\"\n",
        "\n",
        "        score_list = []\n",
        "        for hyp, ref in zip(hypothesis, reference):\n",
        "            hyp = self.clean_text(hyp)\n",
        "            ref = self.clean_text(ref)\n",
        "\n",
        "            # compute scores and match\n",
        "            scores = self.rouge.score(ref, hyp)\n",
        "            bleu = sentence_bleu([ref.split()], hyp.split())\n",
        "            score_dict = {\n",
        "                \"rouge1\": scores[\"rouge1\"].fmeasure,\n",
        "                \"rouge2\": scores[\"rouge2\"].fmeasure,\n",
        "                \"rougeL\": scores[\"rougeL\"].fmeasure,\n",
        "                \"length\": len(word_tokenize(hyp)),\n",
        "                \"bleu-4\": bleu\n",
        "            }\n",
        "            score_list.append(score_dict)\n",
        "        \n",
        "        # compute mean and normalize scores\n",
        "        score_dict = {\n",
        "            \"length\": np.mean([d[\"length\"] for d in score_list]),\n",
        "        }\n",
        "        for metric in self.metric_list:\n",
        "            score_dict[metric] = np.mean([d[metric] for d in score_list])\n",
        "            score_dict[f\"{metric}_normalized\"] = score_dict[metric] / self.normalization_funcs[metric](score_dict[\"length\"])\n",
        "        score_dict[\"bleu-4\"] = np.mean([d[\"bleu-4\"] for d in score_list])\n",
        "        return score_dict\n",
        "        \n",
        "def evaluate(test_annotation_file, user_submission_file, **kwargs):\n",
        "    print(\"Starting Evaluation.....\")\n",
        "    #for metric, score in cocoEval.eval.items():\n",
        "    #    print(metric, score)\n",
        "\n",
        "    # load data\n",
        "    ground_truth = json_load(test_annotation_file)\n",
        "    submission = json_load(user_submission_file)\n",
        "    \n",
        "    print(\"align annotations and submission file..\")\n",
        "    \n",
        "    \n",
        "    # align data\n",
        "    data = {sample[\"image_id\"]: sample for sample in ground_truth[\"annotations\"]}\n",
        "    for prediction in submission:\n",
        "        image_id = prediction[\"image_id\"]\n",
        "        data[image_id][\"prediction\"] = prediction[\"caption\"]\n",
        "\n",
        "    # convert to list\n",
        "    hypothesis = []\n",
        "    reference = []\n",
        "    print(data)\n",
        "    for image_id, sample in data.items():\n",
        "        try:\n",
        "            hypothesis.append(sample[\"prediction\"])\n",
        "            reference.append(sample[\"caption\"])\n",
        "        except:\n",
        "          print(\"Missing id: \", image_id)\n",
        "          return\n",
        "          \n",
        "    assert len(ground_truth['images']) == len(submission), \"The number of submission samples must be the same as the annotated samples.\"\n",
        "    # evaluate\n",
        "    evaluator = ScicapSummarizationEval()\n",
        "    score = evaluator.evaluate(\n",
        "        hypothesis=hypothesis,\n",
        "        reference=reference,\n",
        "    )\n",
        "\n",
        "    output = {}\n",
        "    output[\"result\"] = [\n",
        "        {\n",
        "            \"test_split\": {\n",
        "                \"BLEU-4\": score['bleu-4'],\n",
        "                \"Rouge-1\": score['rouge1'],\n",
        "                \"Rouge-1-normalized\": score['rouge1_normalized'],\n",
        "                \"Rouge-2\": score['rouge2'],\n",
        "                \"Rouge-2-normalized\": score['rouge2_normalized'],\n",
        "                \"Rouge-L\": score['rougeL'],\n",
        "                \"Rouge-L-normalized\": score['rougeL_normalized'],\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "    # To display the results in the result file\n",
        "    output[\"submission_result\"] = output[\"result\"][0]\n",
        "    print(\"Completed evaluation!\")\n",
        "    return output"
      ],
      "metadata": {
        "id": "dOl_KlfOIdCe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a sample for the evaluation, please replace the files with your own prediction and the corresponding data split\n",
        "evaluate(\"/content/test_annotations_devsplit.json\", \"/content/submission_dev.json\")"
      ],
      "metadata": {
        "id": "oz3jBiEBIfer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53610145-29a3-437d-f275-7b8fc5af906c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Evaluation.....\n",
            "align annotations and submission file..\n",
            "{7424363: {'image_id': 7424363, 'id': 8492163, 'caption': 'Fig. 1. The coefficient of Pe2 in the small-Pe optimal enhancement (32) with n = 1 (first zero). The optimal enhancement is achieved for m = 2, then drops off slowly.', 'caption_no_index': 'The coefficient of Pe2 in the small-Pe optimal enhancement (32) with n =  (first zero). The optimal enhancement is achieved for m = 2, then drops off slowly.', 'paragraph': ['Since j m,n increases monotonically with n, we must take n = 1 to minimize T . Thus the optimal streamlines pattern displays a single cell in the radial direction. It is then a simple matter of enumerating the zeros j m,1 to find that the integrated mean exit time is minimized for (m, n) = (2, 1), independent of the Péclet number (see Figure 1). The streamline and mean exit time patterns are illustrated in Figure 2.'], 'mention': [['It is then a simple matter of enumerating the zeros j m,1 to find that the integrated mean exit time is minimized for (m, n) = (2, 1), independent of the Péclet number (see Figure 1).']], 'prediction': 'The optimal enhancement is achieved for m = 2, then drops off slowly.'}}\n",
            "Completed evaluation!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'result': [{'test_split': {'BLEU-4': 0.25042009669367926,\n",
              "    'Rouge-1': 0.5714285714285715,\n",
              "    'Rouge-1-normalized': 3.213721570521792,\n",
              "    'Rouge-2': 0.5499999999999999,\n",
              "    'Rouge-2-normalized': 9.639262673254814,\n",
              "    'Rouge-L': 0.5714285714285715,\n",
              "    'Rouge-L-normalized': 3.8856673511349342}}],\n",
              " 'submission_result': {'test_split': {'BLEU-4': 0.25042009669367926,\n",
              "   'Rouge-1': 0.5714285714285715,\n",
              "   'Rouge-1-normalized': 3.213721570521792,\n",
              "   'Rouge-2': 0.5499999999999999,\n",
              "   'Rouge-2-normalized': 9.639262673254814,\n",
              "   'Rouge-L': 0.5714285714285715,\n",
              "   'Rouge-L-normalized': 3.8856673511349342}}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UHFzQwVhrUbo"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}